# Self-Supervised Image Classification with Contrastive Learning

PyTorch implementation of self-supervised image classification using contrastive learning and iterative pseudo-labeling on STL-10 dataset.

## üéØ Project Goal

Build a classification system that learns to categorize images **without using ground-truth labels during training**, achieving >70% accuracy through self-supervised learning with contrastive pre-training.

## üìä Current Dataset: STL-10

**STL-10** is the primary dataset for this project - specifically designed for self-supervised learning:
- **96√ó96 RGB** images (vs 28√ó28 grayscale in Fashion-MNIST)
- **10 classes**: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck
- **100,000 unlabeled images** for contrastive learning
- **5,000 training + 8,000 test** labeled images for evaluation

### Why STL-10?
‚úÖ **High resolution** ‚Üí More discriminative features
‚úÖ **RGB colors** ‚Üí Can use color augmentation (jittering)
‚úÖ **Distinct classes** ‚Üí Visually different objects (vs similar clothing items)
‚úÖ **Unlabeled data** ‚Üí Ideal for contrastive learning (100k images!)
‚úÖ **Standard benchmark** ‚Üí Used in self-supervised learning research

*Note: Fashion-MNIST support is still available but STL-10 is recommended.*

## üèóÔ∏è Architecture

### Overall Pipeline
```
Phase 0: Contrastive Pre-training (100k unlabeled images)
  ‚Üì
Phase 1-N: Iterative Pseudo-Labeling
  Feature Extraction ‚Üí Clustering ‚Üí Classifier Training ‚Üí Repeat
```

### Contrastive Pre-training (SimCLR-style)
```
Image ‚Üí [Augmentation 1] ‚Üí View 1 ‚îÄ‚îê
                                     ‚îú‚Üí Contrastive Loss ‚Üí Learn Features
Image ‚Üí [Augmentation 2] ‚Üí View 2 ‚îÄ‚îò

Augmentations: RandomResizedCrop, ColorJitter, Flip, Blur, Rotation
Temperature: 0.5 | Epochs: 100 | Unlabeled: 100k images
```

### Iterative Pseudo-Labeling
```
Input (96√ó96 RGB) ‚Üí STL10Encoder (CNN) ‚Üí 512-dim features (L2-normalized)
                                             ‚Üì
                                       K-means Clustering
                                             ‚Üì
                                       Pseudo-labels (0-9)
                                             ‚Üì
                             Classification Head ‚Üí Class Predictions
```

### STL10Encoder Architecture
- **4 Conv blocks**: 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 channels
- BatchNorm + ReLU + MaxPool
- Global Average Pooling
- Output: 512-dim L2-normalized features
- **Parameters**: 4.7M

## üìÅ Project Structure

```
toy/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ stl10_config.yaml        # STL-10 configuration (default)
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml              # Fashion-MNIST configuration
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ STL10/                   # STL-10 dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_images/        # 5,000 images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_images/         # 8,000 images
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ unlabeled_images/    # 100,000 images (contrastive learning)
‚îÇ   ‚îî‚îÄ‚îÄ FashionMNIST/           # Fashion-MNIST dataset (legacy)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ stl10_encoder.py        # STL-10 encoder (96√ó96 RGB)
‚îÇ   ‚îú‚îÄ‚îÄ feature_extractor.py    # Fashion-MNIST encoder (28√ó28 grayscale)
‚îÇ   ‚îú‚îÄ‚îÄ classifier.py           # Classification head
‚îÇ   ‚îî‚îÄ‚îÄ contrastive.py          # Contrastive learning components
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ stl10_loader.py         # STL-10 data loading
‚îÇ   ‚îú‚îÄ‚îÄ stl10_augmentation.py   # RGB image augmentations
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py          # Fashion-MNIST data loading
‚îÇ   ‚îú‚îÄ‚îÄ augmentation_v2.py      # Fashion-MNIST augmentations
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py           # K-means with GPU support
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py              # Hungarian matching, NMI, Purity
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py        # Plotting utilities
‚îú‚îÄ‚îÄ train.py                    # Main training script (auto-detects dataset)
‚îú‚îÄ‚îÄ CHANGES.md                  # Recent changes and migration guide
‚îú‚îÄ‚îÄ STL10_SETUP.md             # Detailed STL-10 setup guide
‚îî‚îÄ‚îÄ README.md                   # This file
```

## üöÄ Quick Start

### 1. Install Dependencies

```bash
pip install torch torchvision pyyaml scikit-learn scipy tqdm pillow kaggle
```

**Optional (Recommended):** Install FAISS for GPU-accelerated clustering:
```bash
pip install faiss-gpu
```

### 2. Download STL-10 Dataset

Already downloaded and placed in `dataset/STL10/`!

To download manually:
```bash
kaggle datasets download -d jessicali9530/stl10
unzip stl10.zip -d dataset/STL10/
```

### 3. Train the Model

```bash
python train.py
```

The script automatically:
- Loads `config/stl10_config.yaml`
- Runs contrastive pre-training on 100k unlabeled images (100 epochs)
- Performs iterative pseudo-labeling (30 iterations)
- Saves checkpoints to `checkpoints_stl10/`

**Configuration:** Edit `config/stl10_config.yaml` to adjust:
- `pretrain_epochs`: Contrastive pre-training epochs (default: 100)
- `num_iterations`: Refinement cycles (default: 30)
- `epochs_per_iteration`: Classifier training epochs (default: 15)
- `batch_size`: Batch size (default: 256)
- `temperature`: Contrastive learning temperature (default: 0.5)

### 4. Monitor Training

```bash
# Training creates these directories:
checkpoints_stl10/  # Model checkpoints
results_stl10/      # Results and plots
logs_stl10/         # Training logs
```

## üìà Expected Performance

### STL-10 (Current)
| Iteration | Accuracy | NMI | Purity | Status |
|-----------|----------|-----|--------|--------|
| 1 | 45-55% | 0.45-0.55 | 0.50-0.60 | Post contrastive learning |
| 5-10 | 60-70% | 0.55-0.65 | 0.65-0.75 | Improving |
| 15-20 | **70-80%** | **0.65-0.75** | **0.75-0.85** | **Target** |
| 25-30 | **75-85%** | **0.70-0.80** | **0.80-0.90** | **Converged** |

### Fashion-MNIST (Legacy - Not Recommended)
| Iteration | Accuracy | NMI | Purity | Status |
|-----------|----------|-----|--------|--------|
| Final | 30-40% | 0.25-0.35 | 0.30-0.40 | Limited by dataset |

**Why Fashion-MNIST struggles:**
- ‚ùå Low resolution (28√ó28)
- ‚ùå Grayscale only (no color augmentation)
- ‚ùå Similar classes (T-shirt vs Shirt vs Pullover)
- ‚ùå No unlabeled data for contrastive learning

## ‚úÖ Success Criteria

### STL-10 Targets
- ‚úÖ **Overall Accuracy > 75%** (vs supervised ~94%)
- ‚úÖ **NMI > 0.70** (clustering-class alignment)
- ‚úÖ **Purity > 0.80** (cluster homogeneity)
- ‚úÖ **All Per-Class F1 > 0.65** (no class left behind)
- ‚úÖ **Label Stability > 0.98** (convergence)

## üîë Key Implementation Details

### 1. Contrastive Pre-training (Critical!)
```python
# Two augmented views of same image
view1, view2 = augment(image), augment(image)

# Project to contrastive space
z1, z2 = projector(encoder(view1)), projector(encoder(view2))

# NT-Xent Loss (Temperature-scaled)
loss = -log(exp(sim(z1, z2) / œÑ) / Œ£ exp(sim(z1, zk) / œÑ))
```
- Learns invariant features before clustering
- Uses 100k unlabeled STL-10 images
- Temperature œÑ = 0.5 for balanced learning

### 2. Strong Augmentation for RGB
```python
RandomResizedCrop(96, scale=(0.2, 1.0))  # SimCLR-style
ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)  # RGB!
RandomHorizontalFlip(p=0.5)  # OK for natural images
GaussianBlur(kernel_size=9, p=0.5)
RandomRotation(degrees=15)
```

### 3. L2 Feature Normalization
```python
features = F.normalize(features, p=2, dim=1)
```
- Applied after encoder, before clustering
- Ensures cosine similarity clustering

### 4. Hungarian Algorithm for Evaluation
```python
from scipy.optimize import linear_sum_assignment
mapping, accuracy = compute_cluster_to_class_mapping(pseudo_labels, true_labels)
```
- Optimal cluster-to-class assignment
- Cluster IDs don't match class IDs

### 5. GPU-Accelerated Clustering
- **FAISS** (recommended): 10x faster for 100k samples
- **Fallback**: sklearn K-means if FAISS unavailable

## üìä Outputs

### Checkpoints (STL-10)
- `checkpoints_stl10/best_model.pth`: Model with highest test accuracy
- `checkpoints_stl10/final_model.pth`: Final model after all iterations
- `checkpoints_stl10/model_iter_*.pth`: Per-iteration checkpoints

### Results
- `results_stl10/training_history.npy`: Metrics across iterations
- `results_stl10/confusion_matrix.png`: Final confusion matrix
- `results_stl10/clustering_quality.png`: NMI/Purity over iterations

## üîÑ Switching Between Datasets

### Use STL-10 (Default)
```bash
python train.py  # Uses config/stl10_config.yaml
```

### Use Fashion-MNIST
Edit `train.py` line 303:
```python
config = load_config('config/config.yaml')  # Change from stl10_config.yaml
```

Or edit config file's `dataset_name`:
```yaml
data:
  dataset_name: 'fashion_mnist'  # or 'stl10'
```

## üõ†Ô∏è Troubleshooting

### CUDA Out of Memory
**Solution**: Reduce batch size in `stl10_config.yaml`:
```yaml
data:
  batch_size: 128  # or 64 instead of 256
```

### Slow Contrastive Pre-training
**Solution**: Reduce pre-training epochs:
```yaml
contrastive:
  pretrain_epochs: 50  # instead of 100
```

### Poor Clustering Quality (NMI < 0.4)
**Solution**:
1. Increase contrastive pre-training epochs
2. Check if contrastive loss is decreasing
3. Verify augmentations are working (views should be different)

## üìö STL-10 Classes

0. Airplane ‚úàÔ∏è
1. Bird üê¶
2. Car üöó
3. Cat üê±
4. Deer ü¶å
5. Dog üêï
6. Horse üê¥
7. Monkey üêµ
8. Ship üö¢
9. Truck üöö

## üî¨ Technical Details

**Why Contrastive Learning?**
- Pre-trains encoder on large unlabeled data (100k images)
- Learns invariant features before clustering
- Significantly improves initial clustering quality

**Why STL-10 over Fashion-MNIST?**
- Higher resolution enables more discriminative features
- RGB colors enable color-based augmentation
- Visually distinct classes are easier to cluster
- 100k unlabeled images perfect for contrastive learning

**Key Hyperparameters:**
- `feature_dim=512`: Feature vector size (increased from 256)
- `temperature=0.5`: Contrastive learning temperature
- `pretrain_epochs=100`: Contrastive pre-training duration
- `projection_dim=128`: Contrastive projection head output

## üìñ References

- Contrastive Learning: [SimCLR (Chen et al., 2020)](https://arxiv.org/abs/2002.05709)
- STL-10 Dataset: [Coates et al., 2011](https://cs.stanford.edu/~acoates/stl10/)
- Deep Clustering: [Caron et al., 2018](https://arxiv.org/abs/1807.05520)
- Hungarian Algorithm: [scipy.optimize.linear_sum_assignment](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)

## üìÑ Additional Documentation

- **`STL10_SETUP.md`**: Detailed STL-10 setup and usage guide
- **`CHANGES.md`**: Recent changes and migration guide from Fashion-MNIST
- **`config/stl10_config.yaml`**: All hyperparameters with comments

---

**Built with PyTorch** | **Self-Supervised Learning** | **Contrastive Pre-training** | **STL-10 Dataset**

Last Updated: 2025-12-07 | Project migrated from Fashion-MNIST to STL-10 for better performance
